{
  "term": "What Is a Neural Network?",
  "definition": "Andy Smith is a Certified Financial Planner (CFP\u00ae), licensed realtor and educator with over 35 years of diverse financial management experience. He is an expert on personal finance, corporate finance and real estate and has assisted thousands of clients in meeting their financial goals over his career.\n\nA neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.\n\nNeural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria. The concept of neural networks, which has its roots inartificial intelligence, is swiftly gaining popularity in the development oftrading systems.\n\nNeural networks, in the world of finance,\u00a0assist in the development of such processes as time-series forecasting,algorithmic trading, securities classification, credit risk modeling, and constructing proprietary indicators and pricederivatives.\n\nA neural network works similarly to the human brain\u2019s neural network. A \u201cneuron\u201d in a neural network is a mathematical function that collects and classifies information according to a specific architecture. The network bears a strong resemblance to statistical methods such as curve fitting and regression analysis.\n\nA neural network contains layers of interconnected nodes. Each node is a known as perceptron and is similar to amultiple linear regression. The perceptron feeds the signal produced by a multiple linear regression into an activation function that may be nonlinear.\n\nThough the concept of integrated machines that can think has existed for centuries, there have been the largest strides in neural networks in the past 100 years. In 1943, Warren McCulloch and Walter Pitts from the University of Illinois and the University of Chicago published \"A Logical Calculus of the Ideas Immanent in Nervous Activity\". The research analyzed how the brain could produce complex patterns and could be simplified down to a binary logic structure with only true/false connections.\n\nFrank Rosenblatt from the Cornell Aeronautical Labratory was credited with the development of perceptron in 1958. His research introduced weights to McColloch's and Pitt's work, and Rosenblatt leveraged his work to demonstrate how a computer could use neural networks to detect imagines and make inferences.\n\nEven though there was a dry spell of research (largely due to a dry spell in funding) during the 1970's, Paul Werbos is often credited with the primary contribution during this time in his PhD thesis.Then, Jon Hopfield presented Hopfield Net, a paper on recurrent neural networks in 1982. In addition, the concept of backpropagation resurfaced, and many researchers began to understand its potential for neural nets.\n\nMost recently, more specific neural network projects are being generated for direct purposes. For example, Deep Blue, developed by IBM, conquered the chess world by pushing the ability of computers to handle complex calculations.Though publicly known for beating the world chess champion, these types of machines are also leveraged to discover new medicine, identifyfinancial markettrend analysis, and perform massive scientific calculations.\n\nRecent analysis from the Los Alamos National Library allows analysts to compare different neural networks. The paper is considered an important part in moving towards characterizing the behavior of robust neural networks.\n\nIn a multi-layered perceptron (MLP), perceptrons are arranged in interconnected layers. The input layer collects input patterns. The output layer has classifications or output signals to which input patterns may map. For instance, the patterns may comprise a list of quantities fortechnical indicatorsabout a security; potential outputs could be \u201cbuy,\u201d \u201chold\u201d or \u201csell.\u201d\n\nHidden layers fine-tune the input weightings until the neural network\u2019s margin of error is minimal. It is hypothesized that hidden layers extrapolate salient features in the input data that have predictive power regarding the outputs. This describes feature extraction, which accomplishes a utility similar to statistical techniques such as principal component analysis.\n\nFeed-forward neural networks are one of the more simple types of neural networks. It conveys information in one direction through input nodes; this information continues to be processed in this single direction until it reaches the output mode. Feed-forward neural networks may have hidden layers for functionality, and this type of most often used for facial recognition technologies.\n\nA more complex type of neural network, recurrent neural networks take the output of a processing node and transmit the information back into the network. This results in theoretical \"learning\" and improvement of the network. Each node stores historical processes, and these historical processes are reused in the future during processing.\n\nThis becomes especially critical for networks in which the prediction is incorrect; the system willattempt to learnwhy the correct outcome occurred and adjust accordingly. This type of neural network is often used in text-to-speech applications.\n\nConvolutional neural networks, also called ConvNets or CNNs, have several layers in which data is sorted into categories. These networks have an input layer, an output layer, and a hidden multitude of convolutional layers in between. The layers create feature maps that record areas of an image that are broken down further until they generate valuable outputs. These layers can be pooled or entirely connected, and these networks are especially beneficial for image recognition applications.\n\nDeconvolutional neural networks simply work in reverse of convolutional neural networks. The application of the network is to detect items that might have been recognized as important under a convolutional neural network. These items would likely have been discarded during the convolutional neural network execution process. This type of neural network is also widely used for image analysis or processing.\n\nModular neural networks contain several networks that work independently from one another. These networks do not interact with each other during an analysis process. Instead, these processes are done to allow complex, elaborate computing processes to be done more efficiently. Similar to other modular industries such asmodular real estate, the goal of the network independence is to have each module responsible for a particular part of an overall bigger picture.\n\nNeural networks are broadly used, with applications for financial operations, enterprise planning, trading, business analytics, and product maintenance. Neural networks have also gained widespread adoption in business applications such as forecasting and marketing research solutions, fraud detection, andrisk assessment.\n\nA neural network evaluates price data and unearths opportunities for making trade decisions based on the data analysis. The networks can distinguish subtle nonlinear interdependencies and patterns other methods oftechnical analysiscannot. According to research, the accuracy of neural networks in making price predictions for stocks differs. Some models predict the correct stock prices 50 to 60% of the time. Still, others have posited that a 10% improvement in efficiency is all an investor can ask for from a neural network.\n\nSpecific to finance, neural networks can process hundreds of thousands of bits of transaction data. This can translate to a better understanding of trading volume, trading range, correlation between assets, or setting volatility expectations for certain investments. As a human may not be able to efficiently pour through years of data (sometimes collected down second intervals), neural networks can be designed to spot trends, analyze outcomes, and predict future asset class value movements.\n\nThere will always be data sets and task classes that a better analyzed by using previously developed algorithms. It is not so much thealgorithmthat matters; it is the well-prepared input data on the targeted indicator that ultimately determines the level of success of a neural network.\n\nNeutral networks that can work continuously and are more efficient than humans or simpler analytical models. Neural networks can also be programmed to learn from prior outputs to determine future outcomes based on the similarity to prior inputs.\n\nNeural networks that leverage cloud of online services also have the benefit of risk mitigation compared to systems that rely on localtechnology hardware. In addition, neural networks can often perform multiple tasks simultaneously (or at least distribute tasks to be performed by modular networks at the same time).\n\nLast, neural networks are continually being expanded into new applications. While early, theoretical neural networks were very limited to its applicability into different fields, neural networks today are leveraged in medicine, science, finance, agriculture, or security.\n\nThough neutral networks may rely on online platforms, there is still a hardware component that is required to create the neural network. This creates a physical risk of the network that relies on complex systems, set-up requirements, and potential physical maintenance.\n\nThough the complexity of neural networks is a strength, this may mean it takes months (if not longer) to develop a specific algorithm for a specific task. In addition, it may be difficult to spot any errors or deficiencies in the process, especially if the results are estimates or theoretical ranges.\n\nNeural networks may also be difficult to audit. Some neural network processes may feel \"like a black box\" where input is entered, networks perform complicated processes, and output is reported. It may also be difficult for individuals to analyze weaknesses within the calculation or learning process of the network if the network lacks generaltransparencyon how a model learns upon prior activity.\n\nCan often work more efficiently and for longer than humans\n\nCan be programmed to learn from prior outcomes to strive to make smarter future calculations\n\nOften leverage online services that reduce (but do not eliminate) systematic risk\n\nAre continually being expanded in new fields with more difficult problems\n\nStill rely on hardware that may require labor and expertise to maintain\n\nMay take long periods of time to develop the code and algorithms\n\nMay be difficult to assess errors or adaptions to the assumptions if the system is self-learning but lacks transparency\n\nUsually report an estimated range or estimated amount that may not actualize\n\nThere are three main components: an input later, a processing layer, and an output layer. The inputs may be weighted based on various criteria. Within the processing layer, which is hidden from view, there are nodes and connections between these nodes, meant to be analogous to the neurons and synapses in an animal brain.\n\nAlso known as a deep learning network, a deep neural network, at its most basic, is one that involves two or more processing layers. Deep neural networks rely on machine learning networks that continually evolve by compared estimated outcomes to actual results, then modifying future projections.\n\nAll neural networks have three main components. First, the input is the data entered into the network that is to be analyzed. Second, the processing layer utilizes the data (and prior knowledge of similar data sets) to formulate an expected outcome. That outcome is the third component, and this third component is the desired end product from the analysis.\n\nNeural networks are complex, integrated systems that can perform analytics much deeper and faster than human capability. There are different types of neural networks, often best suited for different purposes and target outputs. In finance, neural networks are used to analyze transaction history, understand asset movement, and predict financial market outcomes.\n\nIBM. \"What Are Neural Networks?\"\n\nMcculloch, Warren S. and Pitts, Walter. \"A Logical Calculus of the Ideas Immanent in Nervous Activity.\"Bulletin of Mathematical Biophysics, vol. 5, 1943, pp. 115-133.\n\nRosenblatt, Frank. \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.\"Psychological Review, vol. 65, no. 6, 1958, pp. 386-408.\n\nWerbos, Paul. \"Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.\"PhD Thesis, Harvard University, January 1974.\n\nYi, Zhang and Tan, K.K. \"Hopfield Recurrent Neural Networks.\"Convergence Analysis of Recurrent Neural Networks, vol. 13, 2004, pp. 15\u201332.\n\nJones, Haydn and et al. \"If You've Trained One, You've Trained Them All: Inter-Architecture Similarity Increases With Robustness.\"38th Conference on Uncertainty in Artificial Intelligence, 2022.\n\nScienceDirect. \"Multilayer Perceptron.\"\n\nUniversity of Toronto, Department of Computer Science. \"Roger Grosse; Lecture 5: Multilayer Perceptrons.\" Pages 2-3.\n\nScienceDirect. \"Feedforward Neural Network.\"\n\nLu, Jing and et al. \"Extended Feed Forward Neural Networks with Random Weights for Face Recognition.\"Neurocomputing, vol. 136, July 2014, pp. 96-102.\n\nIBM. \"What Are Recurrent Neural Networks?\"\n\nIBM. \"What are Convolutional Neural Networks?\"\n\nScienceDirect. \"Deconvolution.\"\n\nAnupam Shukla, Ritu Tiwari, and Rahul Kala. \"Towards Hybrid and Adaptive Computing, A Perspective; Chapter 14, Modular Neural Networks,\" Pages 307-335. Springer Berlin Heidelberg, 2010.\n\nPang, Xiongwen and et al. \"An Innovative Neural Network Approach for Stock Market Prediction.\"The Journal of Supercomputing, vol. 76, no. 1, March 2020, pp. 2098-2118.",
  "url": "https://www.investopedia.com/terms/n/neuralnetwork.asp"
}